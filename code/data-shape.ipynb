{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# path to the folder containing the data\n",
    "path_folder = \"../dataset/ariel-data-challenge-2024/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## axis_info\n",
    "\n",
    "\n",
    "From Kaggle [discussion](https://www.kaggle.com/competitions/ariel-data-challenge-2024/discussion/540555):\n",
    "\n",
    "The AIRS_CH0 integration time indeed alternates between 0.1 and 4.5, as it stands for the sequence of exposure (ultra short, long, ultra short etc). it is used in the calibration notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['AIRS-CH0-axis0-h', 'AIRS-CH0-axis2-um', 'AIRS-CH0-integration_time',\n",
      "       'FGS1-axis0-h'],\n",
      "      dtype='object')\n",
      "AIRS_CH0_axis0_h:  (11250,)\n",
      "AIRS_CH0_axis2_um:  (356,) -> (282,)\n",
      "AIRS_CH0_integration_time:  (11250,)\n",
      "FGS1_axis0_h:  (135000,)\n"
     ]
    }
   ],
   "source": [
    "# load axis_info\n",
    "axis_info = pd.read_parquet(os.path.join(path_folder, \"axis_info.parquet\"))\n",
    "\n",
    "print(axis_info.keys())\n",
    "\n",
    "cut_inf, cut_sup = (39, 321)\n",
    "\n",
    "AIRS_CH0_axis0_h = np.array(axis_info[\"AIRS-CH0-axis0-h\"].dropna())\n",
    "\n",
    "AIRS_CH0_axis2_um = np.array(axis_info[\"AIRS-CH0-axis2-um\"].dropna())\n",
    "AIRS_CH0_axis2_um_cut = AIRS_CH0_axis2_um[cut_inf:cut_sup]\n",
    "\n",
    "AIRS_CH0_integration_time = np.array(axis_info[\"AIRS-CH0-integration_time\"].dropna())\n",
    "\n",
    "FGS1_axis0_h = np.array(axis_info[\"FGS1-axis0-h\"].dropna())\n",
    "\n",
    "print(\"AIRS_CH0_axis0_h: \", AIRS_CH0_axis0_h.shape)\n",
    "print(\"AIRS_CH0_axis2_um: \", AIRS_CH0_axis2_um.shape, \"->\" , AIRS_CH0_axis2_um_cut.shape)\n",
    "print(\"AIRS_CH0_integration_time: \", AIRS_CH0_integration_time.shape)\n",
    "print(\"FGS1_axis0_h: \", FGS1_axis0_h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff_within_exposure\n",
      "mean_diff:  2.777777777777837e-05 std_diff:  4.019715289383054e-16\n",
      "mean (unit in seconds):  0.10000000000000213\n",
      "diff_between_exposure\n",
      "mean_diff:  0.001305555555555555 std_diff:  3.542347252760878e-16\n",
      "mean (unit in seconds):  4.699999999999998\n"
     ]
    }
   ],
   "source": [
    "diff_within_exposure = np.diff(AIRS_CH0_axis0_h)[::2]\n",
    "\n",
    "diff_between_exposure = np.diff(AIRS_CH0_axis0_h)[1::2]\n",
    "\n",
    "print(\"diff_within_exposure\")\n",
    "mean_diff = np.mean(diff_within_exposure)\n",
    "std_diff = np.std(diff_within_exposure)\n",
    "print(\"mean_diff: \", mean_diff, \"std_diff: \", std_diff)\n",
    "print(\"mean (unit in seconds): \", mean_diff * 3600)\n",
    "\n",
    "print(\"diff_between_exposure\")\n",
    "mean_diff = np.mean(diff_between_exposure)\n",
    "std_diff = np.std(diff_between_exposure)\n",
    "print(\"mean_diff: \", mean_diff, \"std_diff: \", std_diff)\n",
    "print(\"mean (unit in seconds): \", mean_diff * 3600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sample_submission\n",
    "\n",
    "total 567 columns: (planet_id * 1) + (wl_id * 283) + (sigma_id * 283)\n",
    "\n",
    "Note: sigma_id looks like the certentity we hold about the model's output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 567)\n",
      "Index(['planet_id', 'wl_1', 'wl_2', 'wl_3', 'wl_4', 'wl_5', 'wl_6', 'wl_7',\n",
      "       'wl_8', 'wl_9',\n",
      "       ...\n",
      "       'sigma_274', 'sigma_275', 'sigma_276', 'sigma_277', 'sigma_278',\n",
      "       'sigma_279', 'sigma_280', 'sigma_281', 'sigma_282', 'sigma_283'],\n",
      "      dtype='object', length=567)\n"
     ]
    }
   ],
   "source": [
    "sample_submission = pd.read_csv(os.path.join(path_folder, \"sample_submission.csv\"))\n",
    "\n",
    "print(sample_submission.shape)\n",
    "print(sample_submission.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train_adc_info/test_adc_info\n",
    "\n",
    "usd to reverse the adc process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_adc_info\n",
      "(673, 6)\n",
      "Index(['planet_id', 'FGS1_adc_offset', 'FGS1_adc_gain', 'AIRS-CH0_adc_offset',\n",
      "       'AIRS-CH0_adc_gain', 'star'],\n",
      "      dtype='object')\n",
      "\n",
      "\n",
      "test_adc_info\n",
      "(1, 6)\n"
     ]
    }
   ],
   "source": [
    "train_adc_info = pd.read_csv(os.path.join(path_folder, \"train_adc_info.csv\"))\n",
    "test_adc_info = pd.read_csv(os.path.join(path_folder, \"test_adc_info.csv\"))\n",
    "\n",
    "print(\"train_adc_info\")\n",
    "print(train_adc_info.shape)\n",
    "print(train_adc_info.keys())\n",
    "\n",
    "print(\"\\n\\ntest_adc_info\")\n",
    "print(test_adc_info.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train_labels\n",
    "\n",
    "total 284 columns: (planet_id * 1) + (wl_id * 283)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(673, 284)\n",
      "Index(['planet_id', 'wl_1', 'wl_2', 'wl_3', 'wl_4', 'wl_5', 'wl_6', 'wl_7',\n",
      "       'wl_8', 'wl_9',\n",
      "       ...\n",
      "       'wl_274', 'wl_275', 'wl_276', 'wl_277', 'wl_278', 'wl_279', 'wl_280',\n",
      "       'wl_281', 'wl_282', 'wl_283'],\n",
      "      dtype='object', length=284)\n"
     ]
    }
   ],
   "source": [
    "train_labels = pd.read_csv(os.path.join(path_folder, \"train_labels.csv\"))\n",
    "\n",
    "print(train_labels.shape)\n",
    "print(train_labels.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wavelengths\n",
    "\n",
    "![Wavelengths](../img/wavelengths.png \"Wavelengths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavelengths = pd.read_csv(os.path.join(path_folder, \"wavelengths.csv\"))\n",
    "# Total 283 columns, wl_1 = 0.705 um represents the FGS1 wavelength\n",
    "# wl_2 - wl_283 (1.9 - 3.9 um) represents the spectral range of the AIRS data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AIRS-CH0_signal\n",
    "\n",
    "original data shape: (training examples, time steps, wavelengths, spacial)\n",
    "\n",
    "shape of each image is wavelengths $\\times$ spacial ($356 \\times 32$), where x-axis represents the frequency dimension (spectrum), and the y-axis represents the spacial dimension of the detector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FGS1_signal\n",
    "\n",
    "original data shape: (training examples, time steps, wavelengths, spacial)\n",
    "\n",
    "shape of each image is spacial $\\times$ spacial ($32 \\times 32$)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_ariel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
